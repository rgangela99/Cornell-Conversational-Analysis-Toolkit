{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/calebchiam/Documents/GitHub/Cornell-Conversational-Analysis-Toolkit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"../..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Hypergraph features for various predictive tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import PairedPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('reddit-corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reddit-corpus']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/calebchiam/.convokit/downloads/reddit-corpus\n"
     ]
    }
   ],
   "source": [
    "corpus = convokit.Corpus(filename=convokit.download('reddit-corpus'))\n",
    "# corpus = convokit.Corpus(filename='reddit-corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<convokit.model.corpus.Corpus at 0x11f7371d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc = convokit.HyperConvo(prefix_len=10, min_thread_len=10, include_root=False)\n",
    "hc.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "remake_cache = True\n",
    "if remake_cache:\n",
    "    with open(\"hyperconvo_feats.p\", \"wb\") as f:\n",
    "        hyperconvo_feats = {}\n",
    "        for convo in corpus.iter_conversations():\n",
    "            hyperconvo_feats.update(convo.meta[\"hyperconvo\"])\n",
    "        pickle.dump(hyperconvo_feats, f)\n",
    "else:\n",
    "    with open(\"hyperconvo_feats.p\", \"rb\") as f:\n",
    "        hyperconvo_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = corpus.utterance_threads(include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the first 10 comments in each thread\n",
    "thread_pfxs = corpus.utterance_threads(prefix_len=10, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 521777\n",
      "Number of Utterances: 2004262\n",
      "Number of Conversations: 84979\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "thread_roots_by_self_post = defaultdict(list)\n",
    "for top_level_comment, thread in threads.items():\n",
    "    rt = thread[next(iter(thread))].root\n",
    "    thread_roots_by_self_post[rt].append(top_level_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperconvo_feats = hc.retrieve_feats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stats = hc.retrieve_motif_pathway_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_counts = hc.retrieve_motif_counts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_motifs = hc.retrieve_motifs(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_motif_count = {thread_id: hc._latent_motif_count(motif_dict, trans=False)[0] for thread_id, motif_dict in threads_motifs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first generate positive and negative examples based on task\n",
    "\n",
    "def generate_pos_neg(task: str, post_to_thread_obj, threads, thread_pfxs):\n",
    "    pos, neg = [], []\n",
    "    if task == \"comment-growth\":\n",
    "        for post_id, thread_roots in post_to_thread_obj.items():\n",
    "            has_pos = [root for root in thread_roots if len(threads[root]) >= 15]\n",
    "            has_neg = [root for root in thread_roots if len(threads[root]) == 10]\n",
    "            \n",
    "            if has_pos and has_neg:\n",
    "                pos.append(random.choice(has_pos))\n",
    "                neg.append(random.choice(has_neg))\n",
    "    elif task == \"commenter-growth\":\n",
    "        for post_id, thread_roots in post_to_thread_obj.items():\n",
    "            has_pos, has_neg = [], []\n",
    "            for root in thread_roots:\n",
    "                if len(threads[root]) >= 20:\n",
    "                    if len(set(c.user.name for c in threads[root].values())) >= \\\n",
    "                        len(set(c.user.name for c in thread_pfxs[root].values())) * 2:\n",
    "                            has_pos.append(root)\n",
    "                    else:\n",
    "                        has_neg.append(root)\n",
    "            if has_pos and has_neg:\n",
    "                pos.append(random.choice(has_pos))\n",
    "                neg.append(random.choice(has_neg))\n",
    "    print(\"- {} positive, {} negative pts for {} task\".format(len(pos), len(neg), task))\n",
    "    \n",
    "    return pos, neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1827 positive, 1827 negative pts for comment-growth task\n"
     ]
    }
   ],
   "source": [
    "pos_comment_growth, neg_comment_growth = generate_pos_neg(\"comment-growth\", \n",
    "                                                          thread_roots_by_self_post,\n",
    "                                                          threads,\n",
    "                                                          thread_pfxs\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 849 positive, 849 negative pts for commenter-growth task\n"
     ]
    }
   ],
   "source": [
    "pos_commenter_growth, neg_commenter_growth = generate_pos_neg(\"commenter-growth\", \n",
    "                                                          thread_roots_by_self_post,\n",
    "                                                          threads,\n",
    "                                                          thread_pfxs\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_motif = deepcopy(hyperconvo_feats)\n",
    "for thread_id, feats in hyperconv_motif.items():\n",
    "    feats.update(motif_counts[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_users(thread):\n",
    "    return len(set(utt.user.name for utt in thread.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_to_usercount = dict()\n",
    "for thread_id in thread_pfxs:\n",
    "    thread_to_usercount[thread_id] = {\"num_users\": get_num_users(thread_pfxs[thread_id])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_usercount = deepcopy(hyperconvo_feats)\n",
    "for thread_id, feats in hyperconv_usercount.items():\n",
    "    feats.update(thread_to_usercount[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifpaths_usercount = deepcopy(path_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_usercount = deepcopy(latent_motif_count)\n",
    "for thread_id, feats in latent_usercount.items():\n",
    "    feats.update(thread_to_usercount[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thread_id, feats in motifpaths_usercount.items():\n",
    "    feats.update(thread_to_usercount[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_paths = deepcopy(hyperconvo_feats)\n",
    "for thread_id, feats in hyperconv_paths.items():\n",
    "    feats.update(path_stats[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_latent = deepcopy(hyperconvo_feats)\n",
    "for thread_id, feats in hyperconv_latent.items():\n",
    "    feats.update(latent_motif_count[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_motifall = deepcopy(hyperconvo_feats)\n",
    "for thread_id, feats in hyperconv_motifall.items():\n",
    "    feats.update(motif_counts[thread_id])\n",
    "    feats.update(path_stats[thread_id])\n",
    "    feats.update(latent_motif_count[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK: comment-growth\n",
      "\n",
      "- 1827 positive, 1827 negative pts for comment-growth task\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.5506\n",
      "Feature set: hyperconvo\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.5506\n",
      "TOP 5 FEATURES\n",
      "prop-nonzero[indegree over C->C mid-thread responses]: 1.245\n",
      "mean[outdegree over C->c responses]: 0.351\n",
      "mean[outdegree over C->C responses]: 0.351\n",
      "mean[indegree over C->C responses]: 0.351\n",
      "norm.max[outdegree over C->c mid-thread responses]: 0.335\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "max[indegree over C->C responses]: -0.375\n",
      "entropy[indegree over C->C mid-thread responses]: -0.376\n",
      "entropy[indegree over C->C responses]: -0.404\n",
      "mean-nonzero[indegree over C->C responses]: -0.520\n",
      "prop-nonzero[indegree over C->C responses]: -1.022\n",
      "\n",
      "Feature set: hyperconv-usercount\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.5506\n",
      "TOP 5 FEATURES\n",
      "prop-nonzero[indegree over C->C mid-thread responses]: 1.246\n",
      "mean[outdegree over C->c responses]: 0.351\n",
      "mean[outdegree over C->C responses]: 0.351\n",
      "mean[indegree over C->C responses]: 0.351\n",
      "norm.max[outdegree over C->c mid-thread responses]: 0.335\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "norm.max[indegree over C->C responses]: -0.375\n",
      "entropy[indegree over C->C mid-thread responses]: -0.376\n",
      "entropy[indegree over C->C responses]: -0.404\n",
      "mean-nonzero[indegree over C->C responses]: -0.520\n",
      "prop-nonzero[indegree over C->C responses]: -1.022\n",
      "\n",
      "Feature set: latentmotif\n",
      "Test accuracy of 0.6284\n",
      "TOP 5 FEATURES\n",
      "NO_EDGE_TRIADS: 0.904\n",
      "OUTGOING_TRIADS: 0.214\n",
      "DIRECIPROCAL_TRIADS: 0.189\n",
      "DIRECIPROCAL_2TO3_TRIADS: 0.088\n",
      "INCOMING_RECIPROCAL_TRIADS: 0.083\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "OUTGOING_RECIPROCAL_TRIADS: -0.052\n",
      "UNIDIRECTIONAL_TRIADS: -0.103\n",
      "SINGLE_EDGE_TRIADS: -0.111\n",
      "DYADIC_TRIADS: -0.145\n",
      "INCOMING_TRIADS: -0.251\n",
      "\n",
      "Feature set: latentmotif-usercount\n",
      "Test accuracy of 0.5902\n",
      "TOP 5 FEATURES\n",
      "num_users: 0.665\n",
      "NO_EDGE_TRIADS: 0.329\n",
      "OUTGOING_TRIADS: 0.184\n",
      "DIRECIPROCAL_TRIADS: 0.156\n",
      "DIRECIPROCAL_2TO3_TRIADS: 0.099\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "OUTGOING_RECIPROCAL_TRIADS: -0.058\n",
      "SINGLE_EDGE_TRIADS: -0.132\n",
      "UNIDIRECTIONAL_TRIADS: -0.135\n",
      "DYADIC_TRIADS: -0.272\n",
      "INCOMING_TRIADS: -0.287\n",
      "\n",
      "Feature set: motifpaths\n",
      "Test accuracy of 0.6120\n",
      "TOP 5 FEATURES\n",
      "('NO_EDGE_TRIADS',): 0.742\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'DIRECTED_CYCLE_TRIADS', 'DIRECTED_CYCLE_1TO3_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS'): 0.261\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_1TO3_TRIADS', 'DIRECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS', 'TRIRECIPROCAL_TRIADS'): 0.234\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'OUTGOING_TRIADS', 'INCOMING_2TO3_TRIADS'): 0.211\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'OUTGOING_TRIADS', 'OUTGOING_3TO1_TRIADS'): 0.203\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_2TO3_TRIADS', 'INCOMING_RECIPROCAL_TRIADS'): -0.081\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS', 'DIRECTED_CYCLE_1TO3_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS', 'TRIRECIPROCAL_TRIADS'): -0.145\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_1TO3_TRIADS', 'DIRECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS', 'TRIRECIPROCAL_TRIADS'): -0.156\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_1TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS'): -0.160\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS'): -0.293\n",
      "\n",
      "Feature set: motifpaths-usercount\n",
      "Test accuracy of 0.5874\n",
      "TOP 5 FEATURES\n",
      "num_users: 0.616\n",
      "('NO_EDGE_TRIADS',): 0.328\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'DIRECTED_CYCLE_TRIADS', 'DIRECTED_CYCLE_1TO3_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS'): 0.253\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_1TO3_TRIADS', 'DIRECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS', 'TRIRECIPROCAL_TRIADS'): 0.233\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'OUTGOING_TRIADS', 'INCOMING_2TO3_TRIADS'): 0.226\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS', 'DIRECTED_CYCLE_1TO3_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS', 'TRIRECIPROCAL_TRIADS'): -0.154\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_1TO3_TRIADS', 'DIRECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS', 'TRIRECIPROCAL_TRIADS'): -0.156\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_1TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS'): -0.159\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'DYADIC_TRIADS'): -0.194\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS'): -0.344\n",
      "\n",
      "Feature set: hyperconv-motif\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.5674\n",
      "TOP 5 FEATURES\n",
      "prop-nonzero[indegree over C->C mid-thread responses]: 1.271\n",
      "mean[outdegree over C->c responses]: 0.517\n",
      "mean[outdegree over C->C responses]: 0.517\n",
      "mean[indegree over C->C responses]: 0.517\n",
      "norm.max[outdegree over C->c mid-thread responses]: 0.325\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "max[indegree over C->C responses]: -0.378\n",
      "entropy[indegree over C->C mid-thread responses]: -0.425\n",
      "entropy[indegree over C->C responses]: -0.460\n",
      "mean-nonzero[indegree over C->C responses]: -0.504\n",
      "prop-nonzero[indegree over C->C responses]: -0.798\n",
      "\n",
      "Feature set: hyperconv-paths\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.5843\n",
      "TOP 5 FEATURES\n",
      "prop-nonzero[indegree over C->C mid-thread responses]: 1.308\n",
      "mean[outdegree over C->c responses]: 0.482\n",
      "mean[outdegree over C->C responses]: 0.482\n",
      "mean[indegree over C->C responses]: 0.482\n",
      "norm.max[outdegree over C->c mid-thread responses]: 0.312\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "entropy[indegree over C->C mid-thread responses]: -0.399\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS'): -0.427\n",
      "mean-nonzero[indegree over C->C responses]: -0.530\n",
      "entropy[indegree over C->C responses]: -0.587\n",
      "prop-nonzero[indegree over C->C responses]: -0.726\n",
      "\n",
      "Feature set: hyperconv-latent\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.5815\n",
      "TOP 5 FEATURES\n",
      "prop-nonzero[indegree over C->C mid-thread responses]: 1.258\n",
      "mean[outdegree over C->c responses]: 0.533\n",
      "mean[outdegree over C->C responses]: 0.533\n",
      "mean[indegree over C->C responses]: 0.533\n",
      "norm.max[outdegree over C->c mid-thread responses]: 0.318\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "max[indegree over C->C responses]: -0.388\n",
      "norm.max[indegree over C->C responses]: -0.388\n",
      "entropy[indegree over C->C mid-thread responses]: -0.410\n",
      "mean-nonzero[indegree over C->C responses]: -0.512\n",
      "prop-nonzero[indegree over C->C responses]: -0.882\n",
      "\n",
      "Feature set: hyperconvo-motifall\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.5871\n",
      "TOP 5 FEATURES\n",
      "prop-nonzero[indegree over C->C mid-thread responses]: 1.307\n",
      "mean[outdegree over C->c responses]: 0.483\n",
      "mean[outdegree over C->C responses]: 0.483\n",
      "mean[indegree over C->C responses]: 0.483\n",
      "norm.max[outdegree over C->c mid-thread responses]: 0.311\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "prop-multiple[outdegree over C->C responses]: -0.312\n",
      "entropy[indegree over C->C mid-thread responses]: -0.399\n",
      "mean-nonzero[indegree over C->C responses]: -0.528\n",
      "entropy[indegree over C->C responses]: -0.594\n",
      "prop-nonzero[indegree over C->C responses]: -0.717\n",
      "\n",
      "Feature set: usercount\n",
      "Test accuracy of 0.6066\n",
      "TOP 5 FEATURES\n",
      "num_users: 0.481\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "num_users: 0.481\n",
      "\n",
      "TASK: commenter-growth\n",
      "\n",
      "- 849 positive, 849 negative pts for commenter-growth task\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.4940\n",
      "Feature set: hyperconvo\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.4940\n",
      "TOP 5 FEATURES\n",
      "entropy[outdegree over C->c mid-thread responses]: 0.586\n",
      "entropy[outdegree over C->C mid-thread responses]: 0.586\n",
      "norm.max[outdegree over c->c mid-thread responses]: 0.544\n",
      "norm.2nd-largest[outdegree over c->c mid-thread responses]: 0.537\n",
      "entropy[indegree over C->C mid-thread responses]: 0.440\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "2nd-largest / max[indegree over C->c responses]: -0.455\n",
      "max[indegree over C->C mid-thread responses]: -0.517\n",
      "2nd-largest / max[indegree over C->C mid-thread responses]: -0.543\n",
      "prop-multiple[indegree over c->c mid-thread responses]: -0.621\n",
      "prop-multiple[indegree over C->c mid-thread responses]: -0.621\n",
      "\n",
      "Feature set: hyperconv-usercount\n",
      "Excluded {} data point(s) that contained NaN values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of 0.4940\n",
      "TOP 5 FEATURES\n",
      "entropy[outdegree over C->c mid-thread responses]: 0.578\n",
      "entropy[outdegree over C->C mid-thread responses]: 0.578\n",
      "norm.max[outdegree over c->c mid-thread responses]: 0.540\n",
      "norm.2nd-largest[outdegree over c->c mid-thread responses]: 0.534\n",
      "entropy[indegree over C->C mid-thread responses]: 0.442\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "2nd-largest / max[indegree over C->c responses]: -0.458\n",
      "max[indegree over C->C mid-thread responses]: -0.527\n",
      "2nd-largest / max[indegree over C->C mid-thread responses]: -0.546\n",
      "prop-multiple[indegree over c->c mid-thread responses]: -0.619\n",
      "prop-multiple[indegree over C->c mid-thread responses]: -0.619\n",
      "\n",
      "Feature set: latentmotif\n",
      "Test accuracy of 0.4824\n",
      "TOP 5 FEATURES\n",
      "INCOMING_1TO3_TRIADS: 0.453\n",
      "SINGLE_EDGE_TRIADS: 0.324\n",
      "DIRECTED_CYCLE_TRIADS: 0.117\n",
      "INCOMING_2TO3_TRIADS: 0.077\n",
      "INCOMING_TRIADS: 0.026\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "OUTGOING_3TO1_TRIADS: -0.078\n",
      "TRIRECIPROCAL_TRIADS: -0.098\n",
      "OUTGOING_TRIADS: -0.155\n",
      "NO_EDGE_TRIADS: -0.238\n",
      "DIRECIPROCAL_TRIADS: -0.509\n",
      "\n",
      "Feature set: latentmotif-usercount\n",
      "Test accuracy of 0.5000\n",
      "TOP 5 FEATURES\n",
      "INCOMING_1TO3_TRIADS: 0.455\n",
      "SINGLE_EDGE_TRIADS: 0.316\n",
      "DIRECTED_CYCLE_TRIADS: 0.115\n",
      "num_users: 0.107\n",
      "INCOMING_2TO3_TRIADS: 0.076\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "OUTGOING_3TO1_TRIADS: -0.078\n",
      "TRIRECIPROCAL_TRIADS: -0.095\n",
      "OUTGOING_TRIADS: -0.162\n",
      "NO_EDGE_TRIADS: -0.328\n",
      "DIRECIPROCAL_TRIADS: -0.511\n",
      "\n",
      "Feature set: motifpaths\n",
      "Test accuracy of 0.4588\n",
      "TOP 5 FEATURES\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_1TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS'): 0.254\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS'): 0.246\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS'): 0.231\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_1TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS'): 0.208\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_1TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS'): 0.203\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS', 'TRIRECIPROCAL_TRIADS'): -0.194\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'OUTGOING_TRIADS'): -0.199\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS', 'DIRECTED_CYCLE_1TO3_TRIADS'): -0.246\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'OUTGOING_TRIADS', 'INCOMING_2TO3_TRIADS', 'INCOMING_RECIPROCAL_TRIADS'): -0.276\n",
      "('NO_EDGE_TRIADS',): -0.318\n",
      "\n",
      "Feature set: motifpaths-usercount\n",
      "Test accuracy of 0.4882\n",
      "TOP 5 FEATURES\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_1TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS'): 0.254\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS'): 0.224\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS'): 0.222\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_1TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS'): 0.209\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_1TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS'): 0.202\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS', 'TRIRECIPROCAL_TRIADS'): -0.193\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'OUTGOING_TRIADS'): -0.209\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS', 'DIRECTED_CYCLE_1TO3_TRIADS'): -0.247\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'OUTGOING_TRIADS', 'INCOMING_2TO3_TRIADS', 'INCOMING_RECIPROCAL_TRIADS'): -0.275\n",
      "('NO_EDGE_TRIADS',): -0.375\n",
      "\n",
      "Feature set: hyperconv-motif\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.5120\n",
      "TOP 5 FEATURES\n",
      "entropy[outdegree over C->c mid-thread responses]: 0.537\n",
      "entropy[outdegree over C->C mid-thread responses]: 0.537\n",
      "norm.2nd-largest[outdegree over c->c mid-thread responses]: 0.510\n",
      "norm.max[outdegree over c->c mid-thread responses]: 0.492\n",
      "entropy[indegree over C->C mid-thread responses]: 0.454\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "2nd-largest / max[indegree over C->c responses]: -0.492\n",
      "2nd-largest / max[indegree over C->C mid-thread responses]: -0.571\n",
      "prop-multiple[indegree over c->c mid-thread responses]: -0.600\n",
      "prop-multiple[indegree over C->c mid-thread responses]: -0.600\n",
      "max[indegree over C->C mid-thread responses]: -0.645\n",
      "\n",
      "Feature set: hyperconv-paths\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.5060\n",
      "TOP 5 FEATURES\n",
      "entropy[outdegree over C->c mid-thread responses]: 0.564\n",
      "entropy[outdegree over C->C mid-thread responses]: 0.564\n",
      "entropy[indegree over C->C mid-thread responses]: 0.547\n",
      "norm.2nd-largest[outdegree over c->c mid-thread responses]: 0.452\n",
      "norm.max[outdegree over c->c mid-thread responses]: 0.442\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "2nd-largest / max[indegree over C->c responses]: -0.486\n",
      "prop-multiple[indegree over c->c mid-thread responses]: -0.580\n",
      "prop-multiple[indegree over C->c mid-thread responses]: -0.580\n",
      "max[indegree over C->C mid-thread responses]: -0.683\n",
      "2nd-largest / max[indegree over C->C mid-thread responses]: -0.720\n",
      "\n",
      "Feature set: hyperconv-latent\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.4880\n",
      "TOP 5 FEATURES\n",
      "entropy[outdegree over C->c mid-thread responses]: 0.525\n",
      "entropy[outdegree over C->C mid-thread responses]: 0.525\n",
      "norm.2nd-largest[outdegree over c->c mid-thread responses]: 0.498\n",
      "norm.max[outdegree over c->c mid-thread responses]: 0.488\n",
      "entropy[indegree over C->C mid-thread responses]: 0.450\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "2nd-largest / max[indegree over C->c responses]: -0.457\n",
      "2nd-largest / max[indegree over C->C mid-thread responses]: -0.580\n",
      "prop-multiple[indegree over c->c mid-thread responses]: -0.587\n",
      "prop-multiple[indegree over C->c mid-thread responses]: -0.587\n",
      "max[indegree over C->C mid-thread responses]: -0.657\n",
      "\n",
      "Feature set: hyperconvo-motifall\n",
      "Excluded {} data point(s) that contained NaN values.\n",
      "Test accuracy of 0.5120\n",
      "TOP 5 FEATURES\n",
      "entropy[outdegree over C->c mid-thread responses]: 0.560\n",
      "entropy[outdegree over C->C mid-thread responses]: 0.560\n",
      "entropy[indegree over C->C mid-thread responses]: 0.548\n",
      "norm.2nd-largest[outdegree over c->c mid-thread responses]: 0.449\n",
      "norm.max[outdegree over c->c mid-thread responses]: 0.437\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "2nd-largest / max[indegree over C->c responses]: -0.489\n",
      "prop-multiple[indegree over c->c mid-thread responses]: -0.579\n",
      "prop-multiple[indegree over C->c mid-thread responses]: -0.579\n",
      "max[indegree over C->C mid-thread responses]: -0.687\n",
      "2nd-largest / max[indegree over C->C mid-thread responses]: -0.721\n",
      "\n",
      "Feature set: usercount\n",
      "Test accuracy of 0.5471\n",
      "TOP 5 FEATURES\n",
      "num_users: 0.134\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "num_users: 0.134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(2019)\n",
    "for task in [\"comment-growth\", \"commenter-growth\"]: #, \"post-deleted\", \"user-deleted\"\n",
    "    print(\"TASK: {}\\n\".format(task))\n",
    "    \n",
    "    pos, neg = generate_pos_neg(task, thread_roots_by_self_post, threads, thread_pfxs)\n",
    "\n",
    "    pp = PairedPrediction()\n",
    "    pp.fit_predict(hyperconvo_feats, pos, neg)\n",
    "    \n",
    "    for feats, name in [(hyperconvo_feats, \"hyperconvo\"),\n",
    "                        (hyperconv_usercount, \"hyperconv-usercount\"),\n",
    "                        (latent_motif_count, \"latentmotif\"),\n",
    "                        (latent_usercount, \"latentmotif-usercount\"),\n",
    "                        (path_stats, \"motifpaths\"),\n",
    "                        (motifpaths_usercount, \"motifpaths-usercount\"),\n",
    "                        (hyperconv_motif, \"hyperconv-motif\"),\n",
    "                        (hyperconv_paths, \"hyperconv-paths\"),\n",
    "                        (hyperconv_latent, \"hyperconv-latent\"),\n",
    "                        (hyperconv_motifall, \"hyperconvo-motifall\"),\n",
    "                        (thread_to_usercount, \"usercount\")\n",
    "                       ]:\n",
    "        print(\"Feature set: {}\".format(name))\n",
    "        pp.fit_predict(feats, pos, neg, test_size=0.2)\n",
    "        feat_names = list(feats[next(iter(feats))].keys()) \n",
    "        pp.print_extreme_coefs(feat_names, num_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_text = dict()\n",
    "for thread_id in pos + neg:\n",
    "    threads_text[thread_id] = {\"text\": \" \".join(utt.text for utt in thread_pfxs[thread_id].values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame.from_dict(threads_text).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split the pos + neg pairs in train, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "pos_neg_train, pos_neg_test = train_test_split(list(zip(pos, neg)), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = [x[0] for x in pos_neg_train]\n",
    "neg_train = [x[1] for x in pos_neg_train] \n",
    "pos_test = [x[0] for x in pos_neg_test]\n",
    "neg_test = [x[1] for x in pos_neg_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW vectorizer on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_df = text_df.loc[pos_train + neg_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_df = text_df.loc[pos_test + neg_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=0.15, max_df=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_arr = cv.fit_transform(train_text_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_transform_df = pd.DataFrame(train_text_arr.toarray(), columns=cv.get_feature_names(), index=train_text_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actually</th>\n",
       "      <th>after</th>\n",
       "      <th>again</th>\n",
       "      <th>against</th>\n",
       "      <th>agree</th>\n",
       "      <th>already</th>\n",
       "      <th>also</th>\n",
       "      <th>always</th>\n",
       "      <th>am</th>\n",
       "      <th>another</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>wrong</th>\n",
       "      <th>www</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dok2j8c</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e06rqka</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dx05x4c</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dr9meeo</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dwcubcp</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         actually  after  again  against  agree  already  also  always  am  \\\n",
       "dok2j8c         0      0      0        2      0        0     0       0   0   \n",
       "e06rqka         1      0      0        1      0        0     2       2   1   \n",
       "dx05x4c         0      1      0        3      0        0     2       0   0   \n",
       "dr9meeo         0      0      0        0      0        0     1       0   0   \n",
       "dwcubcp         0      1      0        0      0        0     0       0   0   \n",
       "\n",
       "         another  ...  world  would  wouldn  wrong  www  yeah  year  years  \\\n",
       "dok2j8c        0  ...      0      0       0      0    0     0     0      0   \n",
       "e06rqka        2  ...      0      0       1      0    0     1     0      0   \n",
       "dx05x4c        0  ...      0      3       0      0    0     0     0      0   \n",
       "dr9meeo        0  ...      0      1       0      0    0     0     1      0   \n",
       "dwcubcp        0  ...      0      1       0      0    0     0     0      0   \n",
       "\n",
       "         yes  your  \n",
       "dok2j8c    0     1  \n",
       "e06rqka    0     5  \n",
       "dx05x4c    0     0  \n",
       "dr9meeo    0     0  \n",
       "dwcubcp    0     0  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_transform_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_arr = cv.transform(text_test['text'])\n",
    "test_text_transform_df = pd.DataFrame(test_text_arr.toarray(), columns=cv.get_feature_names(), index=test_text_df.index)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "def _generate_paired_X_y(feats: DataFrame, pos_ids, neg_ids):\n",
    "\n",
    "    X, y = [], []\n",
    "    flip = True\n",
    "\n",
    "    excluded = 0\n",
    "    for idx in range(len(pos_ids)):\n",
    "        pos_feats = np.array(feats.loc[pos_ids[idx]])\n",
    "        neg_feats = np.array(feats.loc[neg_ids[idx]])\n",
    "\n",
    "        if (np.isnan(pos_feats).any() or np.isnan(neg_feats).any()):\n",
    "            excluded += 1\n",
    "            continue\n",
    "\n",
    "        if flip:\n",
    "            y.append(1)\n",
    "            diff = pos_feats - neg_feats\n",
    "        else:\n",
    "            y.append(0)\n",
    "            diff = neg_feats - pos_feats\n",
    "\n",
    "        X.append(diff)\n",
    "        flip = not flip\n",
    "\n",
    "    if excluded > 0:\n",
    "        print(\"Excluded {} data point(s) that contained NaN values.\".format(excluded))\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = _generate_paired_X_y(train_text_transform_df, pos_train, neg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = _generate_paired_X_y(test_text_transform_df, pos_test, neg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(679, 202)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- cumulative_bow: 0.7555 train, 0.5118 test\n"
     ]
    }
   ],
   "source": [
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"- {}: {:.4f} train, {:.4f} test\".format(\"cumulative_bow\", train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_extreme_coefs(clf, feature_names, num_features: int = 5):\n",
    "    coefs = clf.named_steps['logreg'].coef_[0].tolist()\n",
    "\n",
    "    assert len(feature_names) == len(coefs)\n",
    "\n",
    "    feats_coefs = sorted(list(zip(feature_names, coefs)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print()\n",
    "    print(\"TOP {} FEATURES\".format(num_features))\n",
    "    for ft, coef in feats_coefs[:num_features]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()\n",
    "    print(\"BOTTOM {} FEATURES\".format(num_features))\n",
    "    for ft, coef in feats_coefs[-num_features:]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 20 FEATURES\n",
      "definitely: 0.763\n",
      "we: 0.719\n",
      "used: 0.664\n",
      "stuff: 0.661\n",
      "work: 0.649\n",
      "say: 0.649\n",
      "remember: 0.641\n",
      "feel: 0.631\n",
      "else: 0.629\n",
      "she: 0.613\n",
      "some: 0.590\n",
      "last: 0.584\n",
      "wrong: 0.576\n",
      "found: 0.573\n",
      "ever: 0.571\n",
      "here: 0.547\n",
      "thought: 0.507\n",
      "who: 0.502\n",
      "believe: 0.465\n",
      "really: 0.463\n",
      "\n",
      "BOTTOM 20 FEATURES\n",
      "had: -0.453\n",
      "before: -0.466\n",
      "own: -0.479\n",
      "literally: -0.489\n",
      "op: -0.497\n",
      "use: -0.515\n",
      "either: -0.519\n",
      "www: -0.520\n",
      "great: -0.525\n",
      "person: -0.536\n",
      "maybe: -0.541\n",
      "exactly: -0.541\n",
      "didn: -0.552\n",
      "gt: -0.562\n",
      "someone: -0.569\n",
      "by: -0.635\n",
      "people: -0.662\n",
      "big: -0.685\n",
      "life: -0.725\n",
      "each: -1.090\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_extreme_coefs(clf, cv.get_feature_names(), num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"- {}: {:.4f} train, {:.4f} test\".format(name, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(2019)\n",
    "\n",
    "for task in [\"comment-growth\", \"commenter-growth\"]: #, \"post-deleted\", \"user-deleted\"\n",
    "    print(\"TASK: {}\\n\".format(task))\n",
    "    \n",
    "    pos, neg = generate_pos_neg(task, thread_roots_by_self_post, threads, thread_pfxs)\n",
    "\n",
    "    X, y = generate_paired_features(hyperconvo_feats, pos, neg)\n",
    "    X_motifcnt, y_motifcnt = generate_paired_features(motif_counts, pos, neg)\n",
    "    X_latent, y_latent = generate_paired_features(latent_motif_count, pos, neg)\n",
    "    X_path, y_path = generate_paired_features(path_stats, pos, neg)\n",
    "    X_hcmotif, y_hcmotif = generate_paired_features(hyperconv_motif, pos, neg)\n",
    "    X_hcpath, y_hcpath = generate_paired_features(hyperconv_paths, pos, neg)\n",
    "    X_hclatent, y_hclatent = generate_paired_features(hyperconv_latent, pos, neg)\n",
    "    X_all, y_all = generate_paired_features(hyperconv_motifall, pos, neg)\n",
    "    for X, y, feats, name in [(X, y, hyperconvo_feats, \"hyperconv\"),\n",
    "                       (X_motifcnt, y_motifcnt, motif_counts, \"motifcount\"),\n",
    "                       (X_latent, y_latent, latent_motif_count, \"latentmotif\"),\n",
    "                       (X_path, y_path, path_stats, \"motifpaths\"),\n",
    "                       (X_hcmotif, y_hcmotif, hyperconv_motif, \"hyperconv-motif\"),\n",
    "                       (X_hcpath, y_hcpath, hyperconv_paths, \"hyperconv-paths\"),\n",
    "                       (X_hclatent, y_hclatent, hyperconv_latent, \"hyperconv-latent\"),\n",
    "                       (X_all, y_all, hyperconv_motifall, \"hyperconvo-motifall\")\n",
    "                      ]:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        train_acc = clf.score(X_train, y_train)\n",
    "        test_acc = clf.score(X_test, y_test)\n",
    "        print(\"- {}: {:.4f} train, {:.4f} test\".format(name, train_acc, test_acc))\n",
    "        print_extreme_coefs(clf, feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
